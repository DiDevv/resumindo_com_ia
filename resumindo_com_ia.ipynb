{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# importação de Bibliotecas necessárias"
      ],
      "metadata": {
        "id": "JAC6pcgXDWqZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO2kTO77DTzk",
        "outputId": "61b399ea-502c-41be-83a7-94ed9cda0147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2025.2.19-py3-none-any.whl.metadata (171 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/171.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.9/171.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Downloading yt_dlp-2025.2.19-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2025.2.19\n"
          ]
        }
      ],
      "source": [
        "!pip install yt-dlp openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yt_dlp\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import getpass"
      ],
      "metadata": {
        "id": "EoTdSb6SDmz4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baixando e convertendo vídeo em áudio"
      ],
      "metadata": {
        "id": "M-GWAbTTERt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = input(\"Qual a URL do seu vídeo?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deE3waR2D6xZ",
        "outputId": "f5c0805c-6997-420f-9f5f-fc876b734582"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qual a URL do seu vídeo?https://www.youtube.com/watch?v=5oz5dwHU_mM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ydl_opts = {\n",
        "    'format': 'bestaudio/best',\n",
        "    'postprocessors': [{\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'mp3',\n",
        "        'preferredquality': '64',\n",
        "    }],\n",
        "}\n",
        "\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([url])\n",
        "\n",
        "print(\"Download concluído!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0hdNTBsEEAT",
        "outputId": "a75d1742-1a9f-4886-8be2-305c2ae9f60c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=5oz5dwHU_mM\n",
            "[youtube] 5oz5dwHU_mM: Downloading webpage\n",
            "[youtube] 5oz5dwHU_mM: Downloading tv client config\n",
            "[youtube] 5oz5dwHU_mM: Downloading player d50f54ef\n",
            "[youtube] 5oz5dwHU_mM: Downloading tv player API JSON\n",
            "[youtube] 5oz5dwHU_mM: Downloading ios player API JSON\n",
            "[youtube] 5oz5dwHU_mM: Downloading m3u8 information\n",
            "[info] 5oz5dwHU_mM: Downloading 1 format(s): 251\n",
            "[download] Destination: Intro to Databricks Lakehouse Platform Architecture and Security [5oz5dwHU_mM].webm\n",
            "[download] 100% of   22.76MiB in 00:00:01 at 15.77MiB/s  \n",
            "[ExtractAudio] Destination: Intro to Databricks Lakehouse Platform Architecture and Security [5oz5dwHU_mM].mp3\n",
            "Deleting original file Intro to Databricks Lakehouse Platform Architecture and Security [5oz5dwHU_mM].webm (pass -k to keep)\n",
            "Download concluído!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformando o Áudio em Texto"
      ],
      "metadata": {
        "id": "iXtJACTQJk4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI Api Key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUowlbndEpVr",
        "outputId": "f20905d6-8248-4e1b-dde3-a035518b12d8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI Api Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
      ],
      "metadata": {
        "id": "RazcyrOZEHUN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observação importante: O arquivo deve ter no máximo 25MB (por isso baixei o áudio na pior qualidade possível).\n",
        "\n",
        "Mas, você pode picotar o arquivo original em partes menores, baixar cada parte, resumir essas partes e juntar no final. Maior rolê, mas que dá pra fazer, dá! xD"
      ],
      "metadata": {
        "id": "OFufvnNfKzyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Lembre-se de atualizar o caminho para o local onde o seu arquivo de áudio realmente está, ok?"
      ],
      "metadata": {
        "id": "4mbeMpJhNq10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file= open(\"/content/Intro to Databricks Lakehouse Platform Architecture and Security [5oz5dwHU_mM].mp3\", \"rb\")\n",
        "transcription = client.audio.transcriptions.create(\n",
        "    model=\"whisper-1\",\n",
        "    file=audio_file\n",
        ")"
      ],
      "metadata": {
        "id": "SwJ589RuFCzd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transcription.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj9eMZw1FIAy",
        "outputId": "a0e8eaf5-de9a-4fcd-95a2-212730bf1568"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Databricks Lakehouse Platform Architecture and Security Fundamentals Data Reliability and Performance. In this video, you'll learn about the importance of data reliability and performance on platform architecture, define Delta Lake, and describe how Photon improves the performance of the Databricks Lakehouse platform. First, we'll address why data reliability and performance is important. It is common knowledge that bad data in equals bad data out, so the data used to build business insights and draw actionable conclusions needs to be reliable and clean. While data lakes are a great solution for holding large quantities of raw data, they lack important features for data reliability and quality, often leading them to be called data swamps. Also, data lakes don't often offer as good of performance as that of data warehouses. Some of the problems data engineers may encounter when using a standard data lake include a lack of ACID transaction support, making it impossible to mix updates, appends, and reads, a lack of schema enforcement, creating inconsistent and low-quality data, and a lack of integration with the data catalog resulting in dark data and no single source of truth. These can bring the reliability of the available data in a data lake into question. As for performance, using object storage means data is mostly kept in immutable files, leading to issues such as ineffective partitioning and having too many small files. Partitioning is sometimes used as a poor man's indexing practice by data engineers, leading to hundreds of dev hours lost tuning file sizes to improve performance. In the end, partitioning tends to be ineffective if the wrong field was selected for partitioning or due to high cardinality columns. And because data lakes lack transaction support, appending new data takes the shape of simply adding new files. The small file problem, however, is a known root cause of query performance degradation. The Databricks Lakehouse platform solves these issues with two foundational technologies, Delta Lake and Photon. Delta Lake is a file-based open-source storage format. It provides guarantees for ACID transactions, meaning no partial or corrupted files, scalable data and metadata handling, leveraging Spark to scale out all the metadata processing, handling metadata for petabyte scale tables, audit history and time travel by providing a transaction log with details about every change to data, providing a full audit trail, including the ability to revert to earlier versions for rollbacks or to reproduce experiments, schema enforcement and schema evolution, preventing the insertion of data with the wrong schema while also allowing table schema to be explicitly and safely changed to accommodate ever-changing data. Support for deletes, updates and merges, which is rare for a distributed processing framework to support. This allows Delta Lake to accommodate complex use cases such as change data capture, slowly changing dimension operations and streaming upserts to name a few. And lastly, a unified streaming and batch data processing, allowing data teams to work across a wide variety of data latencies. From streaming data ingestion to batch history backfill to interactive queries, they all work from the start. Delta Lake runs on top of existing data lakes and is compatible with Apache Spark and other processing engines. Delta Lake uses Delta Tables, which are based on Apache Parquet, a common format for structuring data currently used by many organizations. This similarity makes switching from existing Parquet tables to Delta Tables quick and easy. Delta Tables are also usable with semi-structured and unstructured data, providing versioning, reliability, metadata management and time travel capabilities, making these types of data more manageable. The key to all these features and functions is the Delta Lake transaction log. This ordered record of every transaction makes it possible to accomplish a multi-user work environment because every transaction is accounted for. The transaction log acts as a single source of truth so that the Databricks Lakehouse platform always presents users with correct views of the data. When a user reads a Delta Lake table for the first time or runs a new query on an open table, Spark checks the transaction log for new transactions that have been posted to the table. If a change exists, Spark updates the table. This ensures users are working with the most up-to-date information and the user table is synchronized with the master record. It also prevents the user from making divergent or conflicting changes to the table. And finally, Delta Lake is an open-source project, meaning it provides flexibility to your data management infrastructure. You aren't limited to storing data in a single cloud provider and you can truly engage in a multi-cloud system. Additionally, Databricks has a robust partner solution ecosystem, allowing you to work with the right tools for your use case. Next, let's explore Photon. The architecture of the Lakehouse paradigm can pose challenges with the underlying query execution engine for accessing and processing structured and unstructured data. To support the Lakehouse paradigm, the execution engine has to provide the same performance as a data warehouse while still having the scalability of a data lake. And the solution in the Databricks Lakehouse platform architecture for these challenges is Photon. Photon is the next generation query engine. It provides dramatic infrastructure cost savings, where typical customers are seeing up to an 80% total cost of ownership savings over the traditional Databricks runtime, Spark. Photon is compatible with Spark APIs, implementing a more general execution framework for efficient processing of data with support of the Spark APIs. So with Photon, you see increased speed for use cases such as data ingestion, ETL, streaming, data science, and interactive queries directly on your data lake. As Databricks has evolved over the years, query performance has steadily increased. Powered by Spark and thousands of optimization packages as part of the Databricks runtime, Photon offers two times the speed per the TPC-DS one terabyte benchmark compared to the latest DBR versions. Some customers have reported observing significant speedups using Photon on workloads such as SQL-based jobs, Internet of Things use cases, data privacy and compliance, and loading data into Delta and Parquet. Photon is compatible with the Apache Spark DataFrame and SQL APIs to allow workloads to run without having to make any code changes. Photon coordinates work and resources, transparently accelerating portions of SQL and Spark queries without tuning or user intervention. While Photon started out focusing on SQL use cases, it has evolved in scope to accelerate all data and analytics workloads. Photon is the first purpose-built Lakehouse engine that can be found as a key feature for data performance in the Databricks Lakehouse platform. Unified governance and security. In this video, you'll learn about the importance of having a unified governance and security structure, the available security features, Unity catalog and Delta sharing, and the control and data planes of the Databricks Lakehouse platform. While it's important to make high-quality data available to data teams, the more individual access points added to a system, such as users, groups, or external connectors, the higher the risk of data breaches along any of those lines. And any breach has long-lasting negative impacts on a business and their brand. There are several challenges to data and AI governance, such as the diversity of data and AI assets, as data takes many forms beyond files and tables to complex structures such as dashboards, machine learning models, videos, or images. The use of two disparate and incompatible data platforms, where past needs have forced businesses to use data warehouses for BI and data lakes for AI, resulting in data duplication and unsynchronized governance models. The rise of multi-cloud adoption, where each cloud has a unique governance model that requires individual familiarity, and fragmented tool usage for data governance on the Lakehouse, introducing complexity and multiple integration points in the system, leading to poor performance. To address these challenges, Databricks offers the following solutions. Unity Catalog, as a unified governance solution for all data assets. Delta Sharing, as an open solution to securely share live data to any computing platform. And a divided architecture into two planes, control and data. To simplify permissions, avoid data duplication and reduce risk. We'll start by exploring Unity Catalog. Unity Catalog is a unified governance solution for all data assets. Modern Lakehouse systems support fine grained row, column, and view level access control via SQL, query auditing, attribute based access control, data versioning, and data quality constraints and monitoring. Database admins should be familiar with the standard interfaces allowing existing personnel to manage all the data in an organization in a uniform way. In the Databricks Lakehouse platform, Unity Catalog provides a common governance model based on ANSI SQL to define and enforce fine grained access control on all data and AI assets on any cloud. Unity Catalog supplies one consistent model to discover, access, and share data, enabling better native performance, management, and security across clouds. Because Unity Catalog provides centralized governance for data and AI, there is a single source of truth for all user identities and data assets in the Databricks Lakehouse platform. The common metadata layer for cross workspace metadata is at the account level. It provides a single access point with a common interface for collaboration from any workspace in the platform, removing data team silos. Unity Catalog allows you to restrict access to certain rows and columns to users or groups authorized to query them. And with attribute based access control, you can further simplify governance at scale by controlling access to multiple data items at one time. For example, personally identifiable information in multiple given columns can be tagged as such, and a single rule can restrict or provide access as needed. Regulatory compliance is putting pressure on businesses for full compliance, and data access audits are critical to ensure these regulations are being met. For this, Unity Catalog provides a highly detailed audit trail logging who has performed what action against the data. To break down data silos and democratize data across your organization for data driven decisions, Unity Catalog has a user interface for data search and discovery, allowing teams to quickly search for relevant data assets for any use case. Also, the low latency metadata serving and auto tuning of tables enables Unity Catalog to provide 38 times faster metadata processing compared to Hive Metastore. All the transformations and refinements of data from source to insights is encompassed in data lineage. All of the interactions with the data, including where it came from, what other data sets it might have been combined with, who created it and when, what transformations were performed, and other events and attributes are included in a data sets data lineage. Unity Catalog provides automated data lineage charts down to table and column level, giving that end-to-end view of the data. Not limited to just one workload, multiple data teams can quickly investigate errors in their data pipelines or end applications. Impact analysis can also be performed to identify dependencies of data changes on downstream systems or teams and then notified of potential impacts to their work. And with this power of data lineage, there is an increased understanding of the data, reducing tribal knowledge. And to round it out, Unity Catalog integrates with existing tools to help you future-proof your data and AI governance. Next, we'll discuss data sharing with delta sharing. Data sharing is an important aspect of the digital economy that has developed with the advent of big data. But data sharing is difficult to manage. Existing data sharing technologies come with several limitations. Traditional data sharing technologies do not scale well and often serve files offloaded to a server. Cloud object stores operate on an object level and are cloud specific and commercial data sharing offerings and vendor products often share tables instead of files. Scaling is expensive and they aren't open, therefore don't permit data sharing to a different platform. To address these challenges and limitations, Databricks developed delta sharing with contributions from the OSS community and donated it to the Linux Foundation. It is an open source solution to share live data from your lake house to any computing platform securely. Recipients don't have to be on the same cloud or even use the Databricks Lakehouse platform and the data isn't simply replicated or moved. Additionally, data providers still maintain management and governance of the data with the ability to track and audit usage. Some key benefits of delta sharing include that it is an open cross-platform sharing tool, easily allowing you to share existing data in Delta Lake and Apache Parquet formats without having to establish new ingestion processes to consume data since it provides native integration with Power BI, Tableau, Spark, Pandas, and Java. Data is shared live without copying it, with data being maintained on the provider's data lake, ensuring the data sets are reliable in real-time and provide the most current information to the data recipient. As mentioned earlier, delta sharing provides centralized administration and governance to the data provider as the data is governed, tracked, and audited from a single location, allowing usage to be monitored at the table, partition, and version level. With delta sharing, you can build and package data products through a central marketplace for distribution to anywhere, and it is safe and secure with privacy-safe data clean rooms, meaning collaboration between data providers and recipients is hosted in a secure environment while safeguarding data privacy. Unity Catalog natively supports delta sharing, making these two tools smart choices in your data and AI governance and security structure. Delta sharing is a simple REST protocol that securely shares access to part of a cloud data set. Leveraging modern cloud storage systems, it can reliably transfer large data sets. Finally, let's talk about the security structure of the data lakehouse platform. A simple and unified approach to data security for the lakehouse is a critical requirement, and the Databricks lakehouse platform provides this by splitting the architecture into two separate planes, the control plane and the data plane. The control plane consists of the managed back-end services that Databricks provides. These live in Databricks' own cloud account and are aligned with whatever cloud service the customer is using, that is AWS, Azure, or GCP. Here, Databricks runs the workspace application and manages notebooks, configuration, and clusters. The data plane is where your data is processed. Unless you choose to use serverless compute, the compute resources in the data plane run inside the business owner's own cloud account. All the data stays where it is. While some data such as notebooks, configurations, logs, and user information are available in the control plane, the information is encrypted at REST and communication to and from the control plane is encrypted in transit. Security of the data plane within your chosen cloud service provider is very important, so the Databricks lakehouse platform has several security key points. For the networking of that environment, if the business decides to host the data plane, Databricks will configure the networking by default. The serverless data plane networking infrastructure is managed by Databricks in a Databricks cloud service provider account and shared among customers with additional network boundaries between workspaces and clusters. For servers in the data plane, Databricks clusters are run using the latest hardened system images. Older, less secure images or code cannot be chosen. Databricks code itself is peer-reviewed by security trained developers and extensively reviewed with security in mind. Databricks clusters are typically short-lived, often terminated after a job, and do not persist data after termination. Code is launched in an unprivileged container to maintain system stability. This security design provides protection against persistent attackers and privilege escalation. For Databricks support cases, Databricks access to the environment is limited to cloud service provider APIs for automation and support access. Databricks has a custom-built system allowing our staff access to fix issues or handle support requests, and it requires either a support ticket or an engineering ticket tied expressly to your workspace. Access is limited to a specific group of employees for limited periods of time, and with security audit logs, the initial access event, and the support team members' actions are tracked. For user identity and access, Databricks supports many ways to enable users to access their data. The table ACLs feature uses traditional SQL-based statements to manage access to data and enable fine-grained view-based access. IAM instance profiles enable AWS clusters to assume an IAM role, so users of that cluster automatically access allowed resources without explicit credentials. External storage can be mounted and accessed using a securely stored access key, and the Secrets API separates credentials from code when accessing external resources. As mentioned previously, Databricks provides encryption, isolation, and auditing throughout the governance and security structure. Users can also be isolated at different levels, such as the workspace level, where each team or department uses a different workspace, the cluster level, where cluster ACLs can restrict users who attach notebooks to a given cluster. For high-concurrency clusters, process isolation, JVM whitelisting, and language limitations can be used for safe coexistence of users with different access levels, and single-user clusters, if permitted, allow users to create a private dedicated cluster. And finally, for compliance, Databricks supports these compliance standards on our multi-tenant platform, SoC 2 Type 2, ISO 27001, ISO 27017, and ISO 27018. Certain clouds also support Databricks development options for FedRAMP High, High Trust, HIPAA, and PCI. And Databricks and the Databricks platform are also GDPR and CCPA ready. Instant compute and serverless. In this video, you'll learn about the available compute resources for the Databricks Lakehouse platform, what serverless compute is, and the benefits of Databricks serverless SQL. The Databricks Lakehouse platform architecture is split into the control plane and the data plane. The data plane is where data is processed by clusters of compute resources. This architecture is known as the classic data plane. With the classic data plane, compute resources are run in the business's cloud storage account, and clusters perform distributed data analysis using queries in the Databricks SQL workspace, or notebooks in the data science and engineering or Databricks machine learning environments. However, in using this structure, businesses encountered challenges. First, creating clusters is a complicated task. Choosing the correct size, instance type, and configuration for the cluster can be overwhelming to the user provisioning the cluster. Next, it takes several minutes for the environment to start after making the multitude of choices to configure and provision the cluster. And finally, because these clusters are hosted within the business's cloud account, there are many additional considerations to make about managing the capacity and pool of resources available. And this leads to users exhibiting some costly behaviors, such as leaving clusters running for longer than necessary to avoid the startup times and over-provisioning their resources to ensure the cluster can handle spikes in data processing needs, leading to users paying for unneeded resources and having large amounts of admin overhead, ending up with unproductive users. To solve these problems for the business, Databricks has released the serverless compute option or serverless data plane. As of the release of this content, serverless compute is only available for use with Databricks SQL and is referred to at times as Databricks serverless SQL. Serverless compute is a fully managed service that Databricks provisions and manages the compute resources for a business in the Databricks cloud account instead of the business's. The environment starts immediately, scales up and down within seconds, is completely managed by Databricks. You have clusters available on demand. And when finished, the resources are released back to Databricks. Because of this, the total cost of ownership decreases on average between 20 to 40%. Admin overhead is eliminated and users see an increase in their productivity. At the heart of the serverless compute is a fleet of database clusters that are always running, unassigned to any customer, waiting in a warm state, ready to be assigned within seconds. The pool of resources managed by Databricks so the business doesn't need to worry about the offerings from the cloud service, and Databricks works with the cloud vendors to keep things patched and upgraded as needed. When allocated to the business, the serverless compute resource is elastic, being able to scale up or down as needed, and has three layers of isolation, the container hosting the runtime, the virtual machine hosting the container, and the virtual network for the workspace. And each part is isolated with no sharing or cross-network traffic allowed, ensuring your work is secure. When finished, the VM is terminated and not reused, but entirely deleted, and a new unallocated VM is released back into the pool of waiting resources. Introduction to Lakehouse data management terminology. In this video, you'll learn about the definitions for common Lakehouse terms such as metastore, catalog, schema, table, view, and function, and how they're used to describe data management in the Databricks Lakehouse platform. Delta Lake, a key architectural component of the Databricks Lakehouse platform, provides a data storage format built for the Lakehouse, and Unity Catalog, the data governance solution for the Databricks Lakehouse platform, allows administrators to manage and control access to data. Unity Catalog provides a common governance model to define and enforce fine-grained access control on all data and AI assets on any cloud. Unity Catalog supplies one consistent place for governing all workspaces to discover, access, and share data, enabling better native performance, management, and security across clouds. Let's look at some of the key elements of Unity Catalog that are important to understanding how data management works in Databricks. The metastore is the top-level logical container in Unity Catalog. It's a construct that represents the metadata. Metadata is the information about the data objects being managed by the metastore and the ACLs governing those lists. Compared to the Hive metastore, which is a local metastore linked to each Databricks workspace, Unity Catalog metastores offer improved security and auditing capabilities, as well as other useful features. The next thing in the data object hierarchy is the catalog. A catalog is the topmost container for data objects in Unity Catalog. A metastore can have as many catalogs as desired, although only those with appropriate permissions can create them. Because catalogs constitute the topmost element in the addressable data hierarchy, the catalog forms the first part of the three-level namespace that data analysts use to reference data objects in Unity Catalog. This image illustrates how a three-level namespace compares to a traditional two-level namespace. Analysts familiar with the traditional Databricks or SQL for that matter should recognize the traditional two-level namespace used to address tables within schemas. Unity Catalog introduces a third level to provide improved data segregation capabilities. Complete SQL references in Unity Catalog use three levels. A schema is part of traditional SQL and is unchanged by Unity Catalog. It functions as a container for data assets like tables and views, and is the second part of the three-level namespace referenced earlier. Catalogs can contain as many schemas as desired, which in turn can contain as many data objects as desired. At the bottom layer of the hierarchy are tables, views, and functions. Starting with tables, these are SQL relations consisting of an ordered list of columns. Though Databricks doesn't change the overall concept of a table, tables do have two key variations. It's important to recognize that tables are defined by two distinct elements. First, the metadata, or the information about the table, such as comments, tags, and the list of columns, and associated data types, and then the data that populates the rows of the table. The data originates from formatted data files stored in the business's cloud object storage. There are two types of tables in this structure, managed and external tables. Both tables have metadata managed by the metastore in the control plane. The difference lies in where the table data is stored. With a managed table, data files are stored in the metastore's managed storage location, whereas within an external table, data files are stored in an external storage location. From an access control point of view, managing both types of tables is identical. Views are stored queries executed when you query the view. Views perform arbitrary SQL transformations on tables and other views, and are read-only. They do not have the ability to modify the underlying data. The final element in the data object hierarchy are user-defined functions. User-defined functions enable you to encapsulate custom functionality into a function that can be invoked within queries. Storage credentials are created by admins and are used to authenticate with cloud storage containers, either external storage, user-supplied storage, or the managed storage location for the metastore. External locations are used to provide access control at the file level. Shares and recipients relate to Delta Sharing, an open protocol developed by Databricks for secure, low-overhead data sharing across organizations. It's intrinsically built into Unity Catalog, and is used to explicitly declare shares, read-only logical collections of tables. These can be shared with one or more recipients inside or outside the organization. Shares can be used for two main purposes, to secure shared data outside the organization in a performant way, or to provide linkage between metastores in different parts of the world. The metastore is best described as a logical construct for organizing your data and its associated metadata, rather than a physical container itself. The metastore essentially functions as a reference for a collection of metadata and a link to the cloud storage container. The metadata, information about the data objects and the ACLs for those objects, are stored in the control plane, and data related to objects maintained by the metastore is stored in a cloud storage container.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resumindo o texto!"
      ],
      "metadata": {
        "id": "jEcyuhLJJqk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "  Você é um redator excepcional, com um olhar clínico para detalhes e uma habilidade única para captar todos os pontos importantes.\n",
        "  Sua tarefa é transformar o conteúdo fornecido em um resumo claro, conciso e organizado em Markdown, perfeito para anotações.\n",
        "  Lembre-se: cada detalhe é crucial, pois o resumo será utilizado para referência posterior.\n",
        "  Você pode iniciar o resumo diretamente, sem a necessidade de introduções como \"o vídeo trata de...\".\n",
        "  Foque diretamente nos elementos essenciais e relevantes do texto.\n",
        "\n",
        "  Abaixo, segue o texto a ser resumido:\n",
        "  {transcription.text}\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "GMk-leWhGQYx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model = \"gpt-4o-mini\""
      ],
      "metadata": {
        "id": "eB2MiRftHTFC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=llm_model):\n",
        "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "HhbQkQULHWtJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = get_completion(prompt)\n",
        "resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "hqCy7qmSHnw7",
        "outputId": "5c32eb06-cd02-4243-d312-2b9e97a453b0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Resumo: Databricks Lakehouse Platform Architecture and Security Fundamentals\\n\\n## Importância da Confiabilidade e Desempenho dos Dados\\n- **Confiabilidade dos Dados**: Dados ruins resultam em insights ruins. É crucial que os dados sejam confiáveis e limpos.\\n- **Desempenho**: Data lakes, embora armazenem grandes quantidades de dados, carecem de recursos para garantir a qualidade e confiabilidade, frequentemente sendo chamados de \"data swamps\".\\n- **Problemas com Data Lakes**:\\n  - Falta de suporte a transações ACID.\\n  - Ausência de imposição de esquema, resultando em dados inconsistentes.\\n  - Integração deficiente com catálogos de dados, levando a \"dark data\".\\n\\n## Soluções da Databricks\\n### Delta Lake\\n- **Formato de Armazenamento**: Open-source, baseado em arquivos.\\n- **Garantias**:\\n  - Suporte a transações ACID.\\n  - Manipulação escalável de dados e metadados.\\n  - Histórico de auditoria e \"time travel\".\\n  - Evolução de esquema e imposição de esquema.\\n  - Suporte para operações complexas (ex: captura de dados em mudança).\\n  - Processamento unificado de dados em streaming e em lote.\\n- **Delta Tables**: Baseadas em Apache Parquet, compatíveis com dados semi-estruturados e não estruturados.\\n\\n### Photon\\n- **Motor de Consulta**: Nova geração, otimizado para desempenho.\\n- **Benefícios**:\\n  - Redução de custos de infraestrutura (até 80%).\\n  - Aumento de velocidade em ingestão de dados, ETL, e consultas interativas.\\n  - Compatível com APIs do Spark, sem necessidade de alterações de código.\\n\\n## Governança e Segurança Unificadas\\n### Unity Catalog\\n- **Solução de Governança**: Controle de acesso granular em todos os ativos de dados.\\n- **Recursos**:\\n  - Controle de acesso baseado em atributos.\\n  - Auditoria detalhada de ações sobre dados.\\n  - Interface para busca e descoberta de dados.\\n  - Linhagem de dados automatizada.\\n\\n### Delta Sharing\\n- **Compartilhamento de Dados**: Solução open-source para compartilhar dados ao vivo de forma segura.\\n- **Benefícios**:\\n  - Compartilhamento cross-platform.\\n  - Governança centralizada e rastreamento de uso.\\n\\n## Estrutura de Segurança\\n- **Arquitetura**: Dividida em plano de controle e plano de dados.\\n  - **Plano de Controle**: Serviços gerenciados pela Databricks.\\n  - **Plano de Dados**: Processamento de dados no ambiente do cliente.\\n- **Segurança**:\\n  - Criptografia em repouso e em trânsito.\\n  - Controle de acesso baseado em ACLs.\\n  - Suporte a conformidade com padrões como SoC 2, ISO 27001, GDPR, entre outros.\\n\\n## Computação Instantânea e Serverless\\n### Serverless Compute\\n- **Definição**: Serviço gerenciado que provisiona recursos de computação na conta da Databricks.\\n- **Vantagens**:\\n  - Início imediato e escalabilidade rápida.\\n  - Redução de custos (20-40%).\\n  - Eliminação de sobrecarga administrativa.\\n\\n## Terminologia de Gerenciamento de Dados do Lakehouse\\n- **Metastore**: Contêiner lógico para metadados.\\n- **Catalog**: Contêiner superior para objetos de dados.\\n- **Schema**: Contêiner para ativos de dados (tabelas e views).\\n- **Tables e Views**: Estruturas de dados e consultas armazenadas.\\n- **User-defined Functions**: Funções personalizadas encapsuladas para uso em consultas.\\n\\n## Conclusão\\nA arquitetura e segurança da Databricks Lakehouse Platform são projetadas para garantir confiabilidade, desempenho e governança eficaz dos dados, utilizando tecnologias como Delta Lake e Photon, além de um robusto sistema de governança e segurança.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GG, agora é só colar o resumo no seu Notion ^^ !"
      ],
      "metadata": {
        "id": "HQNbG6SdLFc9"
      }
    }
  ]
}